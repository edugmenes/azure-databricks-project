# End-to-End Azure Data Engineering

This repository contains my **first Data Engineering project**, built using **Microsoft Azure Cloud** and **Azure Databricks**.

The project focuses on designing and implementing **ETL pipelines** using **PySpark** following the **Medallion Architecture (Bronze, Silver, Gold)**, a modern and widely adopted pattern for building scalable and reliable data platforms.



## ğŸš€ Project Overview

The main objective of this project is to demonstrate how raw data can be ingested, transformed, and curated into analytics-ready datasets using cloud-native tools and best practices.

The solution covers:
- Data ingestion from raw sources
- Data transformation and cleansing
- Data modeling for analytics consumption
- Distributed data processing with PySpark



## ğŸ—ï¸ Architecture
![Architecture Diagram](/public/images/Captura%20de%20tela%202026-01-20%20123843.png)

### Azure Event Hubs
Purpose: Real-time data ingestion

Role in project:
- Captures streaming events (clicks, logs, IoT, transactions)
- Highly scalable and fault tolerant

âœ¨ Without Event Hubs: Youâ€™d miss live data or overload systems

### Azure Data Factory (ADF)
Purpose: Orchestration & batch ingestion

Role in project:
- Schedules pipelines
- Moves data from source â†’ data lake
- Triggers Databricks jobs

âœ¨ Think of it as the control center

### Azure Databricks (Apache Spark)
Purpose: Data processing & transformation

Role in project:
- Processes huge volumes of data efficiently
- Implements Bronze â†’ Silver â†’ Gold logic
- Handles both batch and streaming data

âœ¨ This is the engine of the architecture

### Azure Data Lake Storage Gen2 (ADLS)
Purpose: Central storage layer

Role in project:
- Stores all data (Bronze, Silver, Gold)
- Cheap, scalable, secure
- Optimized for analytics

âœ¨ This is your single source of truth

### Delta Lake
Purpose: Reliability & governance on top of ADLS

Role in project:
- ACID transactions
- Schema enforcement
- Time travel (data versioning)
- Efficient reads/writes

âœ¨ Delta Lake turns â€œfilesâ€ into real analytical tables



## ğŸ§® Data Model
The project is structured using a **Medallion Architecture** 

    <storage-account>/<container>/
    â”‚
    â””â”€â”€ projet1/
        â”œâ”€â”€ resources/                  # Source and target data
        â”‚   â”œâ”€â”€ source/                 # CSV files received from the source
        â”‚   â””â”€â”€ target/                 # Exports files for customers
        â”‚
        â”œâ”€â”€ 01-bronze/                  # Raw data
        â”‚   â”œâ”€â”€ customers/
        â”‚   â”‚   â””â”€â”€ customers.parquet
        â”‚   â”œâ”€â”€ sales/
        â”‚   â”‚   â””â”€â”€ sales.parquet
        â”‚   ...
        â”‚    
        â”œâ”€â”€ 02-silver/                  # Clean data
        â”‚   â”œâ”€â”€ customers/
        â”‚   â”‚   â””â”€â”€ customers.parquet
        â”‚   â”œâ”€â”€ sales/
        â”‚   â”‚   â””â”€â”€ sales.parquet
        â”‚   ...
        â”‚
        â”œâ”€â”€ 03-gold/                    # Aggregated data
        â”‚   â”œâ”€â”€ sales_per_category/
        â”‚   â”œâ”€â”€ sales_per_city/
        â”‚   ...
        â”‚
        â”œâ”€â”€ metadata/                   # Metadata and logs
        â”‚   â”œâ”€â”€ bronze/
        â”‚   â”œâ”€â”€ silver/
        â”‚   â”œâ”€â”€ gold/
        â”‚   â”œâ”€â”€ ddl/                    # CREATE TABLE scripts
        â”‚   â”œâ”€â”€ logs/                   # ETL execution logs
        â”‚   â””â”€â”€ checkpoints/            # Autoloader checkpoints / streaming
        â”‚
        â””â”€â”€ tmp/                        # Temporary staging

### ğŸŸ¤ Bronze Layer
- Raw data ingestion
- Minimal transformation
- Preserves source data as-is

### âšª Silver Layer
- Data cleansing and normalization
- Data enrichment
- Application of business rules

### ğŸŸ¡ Gold Layer
- Curated, analytics-ready datasets
- Optimized for reporting and BI use cases



## ğŸ› ï¸ Technologies Used

- Microsoft Azure
- Azure Databricks
- Apache Spark (PySpark)
- Delta Lake
- Medallion Architecture



## ğŸ¯ Key Learnings

Through this project, I gained hands-on experience with:
- Cloud-based data platforms
- Distributed data processing using Spark
- Building scalable ETL pipelines
- Applying modern Data Engineering design patterns
- Managing data across multiple data layers



## ğŸ“Œ Notes

This is a **learning project**, created to apply theoretical concepts in a practical environment using industry-standard tools.

Future improvements may include:
- Pipeline orchestration
- Data quality checks
- Performance optimization
- Monitoring and logging
